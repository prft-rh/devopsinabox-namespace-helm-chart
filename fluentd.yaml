kind: ConfigMap
apiVersion: v1
metadata:
  name: fluentd
  namespace: openshift-logging
  selfLink: /api/v1/namespaces/openshift-logging/configmaps/fluentd
  uid: 4206acd5-3f94-4830-94c7-dc90114b445d
  resourceVersion: '93865758'
  creationTimestamp: '2020-12-16T20:42:14Z'
  ownerReferences:
    - apiVersion: logging.openshift.io/v1
      kind: ClusterLogging
      name: instance
      uid: 1a7c5716-3da5-4155-98ac-468326df3534
      controller: true
data:
  fluent.conf: "\n## CLO GENERATED CONFIGURATION ###\n# This file is a copy of the fluentd configuration entrypoint\n# which should normally be supplied in a configmap.\n\n<system>\n@log_level \"#{ENV['LOG_LEVEL'] || 'warn'}\"\n</system>\n\n# In each section below, pre- and post- includes don't include anything initially;\n# they exist to enable future additions to openshift conf as needed.\n\n## sources\n## ordered so that syslog always runs last...\n<source>\n@type prometheus\nbind ''\n\t<ssl>\n\tenable true\n\tcertificate_path \"#{ENV['METRICS_CERT'] || '/etc/fluent/metrics/tls.crt'}\"\n\tprivate_key_path \"#{ENV['METRICS_KEY'] || '/etc/fluent/metrics/tls.key'}\"\n</ssl>\n</source>\n\n<source>\n@type prometheus_monitor\n\t<labels>\n\thostname ${hostname}\n</labels>\n</source>\n\n# excluding prometheus_tail_monitor\n# since it leaks namespace/pod info\n# via file paths\n\n# This is considered experimental by the repo\n<source>\n@type prometheus_output_monitor\n\t<labels>\n\thostname ${hostname}\n</labels>\n</source>\n\n#journal logs to gather node\n<source>\n@type systemd\n@id systemd-input\n@label @INGRESS\npath \"#{if (val = ENV.fetch('JOURNAL_SOURCE','')) && (val.length > 0); val; else '/run/log/journal'; end}\"\n\t<storage>\n\t@type local\n\tpersistent true\n\t# NOTE: if this does not end in .json, fluentd will think it\n\t# is the name of a directory - see fluentd storage_local.rb\n\tpath \"#{ENV['JOURNAL_POS_FILE'] || '/var/log/journal_pos.json'}\"\n</storage>\nmatches \"#{ENV['JOURNAL_FILTERS_JSON'] || '[]'}\"\ntag journal\nread_from_head \"#{if (val = ENV.fetch('JOURNAL_READ_FROM_HEAD','')) && (val.length > 0); val; else 'false'; end}\"\n</source>\n\n# container logs\n<source>\n@type tail\n@id container-input\npath \"/var/log/containers/*.log\"\nexclude_path [\"/var/log/containers/fluentd-*_openshift-logging_*.log\", \"/var/log/containers/elasticsearch-*_openshift-logging_*.log\", \"/var/log/containers/kibana-*_openshift-logging_*.log\"]\npos_file \"/var/log/es-containers.log.pos\"\nrefresh_interval 5\nrotate_wait 5\ntag kubernetes.*\nread_from_head \"true\"\n@label @CONCAT\n\t<parse>\n\t@type multi_format\n\t\t<pattern>\n\t\tformat json\n\t\ttime_format '%Y-%m-%dT%H:%M:%S.%N%Z'\n\t\tkeep_time_key true\n\t</pattern>\n\t\t<pattern>\n\t\tformat regexp\n\t\texpression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/\n\t\ttime_format '%Y-%m-%dT%H:%M:%S.%N%:z'\n\t\tkeep_time_key true\n\t</pattern>\n</parse>\n</source>\n\n<label @CONCAT>\n\t<filter kubernetes.**>\n\t@type concat\n\tkey log\n\tpartial_key logtag\n\tpartial_value P\n\tseparator ''\n</filter>\n\t<match kubernetes.**>\n\t@type relabel\n\t@label @INGRESS\n</match>\n</label>\n\n#syslog input config here\n\n<label @INGRESS>\n## filters\n\t<filter **>\n\t@type record_modifier\n\tchar_encoding utf-8\n</filter>\n\t<filter journal>\n\t@type grep\n\t\t<exclude>\n\t\tkey PRIORITY\n\t\tpattern ^7$\n\t</exclude>\n</filter>\n\t<match journal>\n\t@type rewrite_tag_filter\n\t# skip to @INGRESS label section\n\t@label @INGRESS\n\t# see if this is a kibana container for special log handling\n\t# looks like this:\n\t# k8s_kibana.a67f366_logging-kibana-1-d90e3_logging_26c51a61-2835-11e6-ad29-fa163e4944d5_f0db49a2\n\t# we filter these logs through the kibana_transform.conf filter\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_kibana\\.\n\t\ttag kubernetes.journal.container.kibana\n\t</rule>\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_[^_]+_logging-eventrouter-[^_]+_\n\t\ttag kubernetes.journal.container._default_.kubernetes-event\n\t</rule>\n\t# mark logs from default namespace for processing as k8s logs but stored as system logs\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_[^_]+_[^_]+_default_\n\t\ttag kubernetes.journal.container._default_\n\t</rule>\n\t# mark logs from kube-* namespaces for processing as k8s logs but stored as system logs\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_[^_]+_[^_]+_kube-(.+)_\n\t\ttag kubernetes.journal.container._kube-$1_\n\t</rule>\n\t# mark logs from openshift-* namespaces for processing as k8s logs but stored as system logs\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_[^_]+_[^_]+_openshift-(.+)_\n\t\ttag kubernetes.journal.container._openshift-$1_\n\t</rule>\n\t# mark logs from openshift namespace for processing as k8s logs but stored as system logs\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_[^_]+_[^_]+_openshift_\n\t\ttag kubernetes.journal.container._openshift_\n\t</rule>\n\t# mark fluentd container logs\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_.*fluentd\n\t\ttag kubernetes.journal.container.fluentd\n\t</rule>\n\t# this is a kubernetes container\n\t\t<rule>\n\t\tkey CONTAINER_NAME\n\t\tpattern ^k8s_\n\t\ttag kubernetes.journal.container\n\t</rule>\n\t# not kubernetes - assume a system log or system container log\n\t\t<rule>\n\t\tkey _TRANSPORT\n\t\tpattern .+\n\t\ttag journal.system\n\t</rule>\n</match>\n\t<filter kubernetes.**>\n\t@type kubernetes_metadata\n\tkubernetes_url \"#{ENV['K8S_HOST_URL']}\"\n\tcache_size \"#{ENV['K8S_METADATA_CACHE_SIZE'] || '1000'}\"\n\twatch \"#{ENV['K8S_METADATA_WATCH'] || 'false'}\"\n\tuse_journal \"#{ENV['USE_JOURNAL'] || 'nil'}\"\n\tssl_partial_chain \"#{ENV['SSL_PARTIAL_CHAIN'] || 'true'}\"\n</filter>\n\n\t<filter kubernetes.journal.**>\n\t@type parse_json_field\n\tmerge_json_log \"#{ENV['MERGE_JSON_LOG'] || 'false'}\"\n\tpreserve_json_log \"#{ENV['PRESERVE_JSON_LOG'] || 'true'}\"\n\tjson_fields \"#{ENV['JSON_FIELDS'] || 'MESSAGE,log'}\"\n</filter>\n\n\t<filter kubernetes.var.log.containers.**>\n\t@type parse_json_field\n\tmerge_json_log \"#{ENV['MERGE_JSON_LOG'] || 'false'}\"\n\tpreserve_json_log \"#{ENV['PRESERVE_JSON_LOG'] || 'true'}\"\n\tjson_fields \"#{ENV['JSON_FIELDS'] || 'log,MESSAGE'}\"\n</filter>\n\n\t<filter kubernetes.var.log.containers.eventrouter-** kubernetes.var.log.containers.cluster-logging-eventrouter-**>\n\t@type parse_json_field\n\tmerge_json_log true\n\tpreserve_json_log true\n\tjson_fields \"#{ENV['JSON_FIELDS'] || 'log,MESSAGE'}\"\n</filter>\n\n\t<filter **kibana**>\n\t@type record_transformer\n\tenable_ruby\n\t\t<record>\n\t\tlog ${record['err'] || record['msg'] || record['MESSAGE'] || record['log']}\n\t</record>\n\tremove_keys req,res,msg,name,level,v,pid,err\n</filter>\n\t<filter **>\n\t@type viaq_data_model\n\tdefault_keep_fields CEE,time,@timestamp,aushape,ci_job,collectd,docker,fedora-ci,file,foreman,geoip,hostname,ipaddr4,ipaddr6,kubernetes,level,message,namespace_name,namespace_uuid,offset,openstack,ovirt,pid,pipeline_metadata,rsyslog,service,systemd,tags,testcase,tlog,viaq_msg_id\n\textra_keep_fields \"#{ENV['CDM_EXTRA_KEEP_FIELDS'] || ''}\"\n\tkeep_empty_fields \"#{ENV['CDM_KEEP_EMPTY_FIELDS'] || 'message'}\"\n\tuse_undefined \"#{ENV['CDM_USE_UNDEFINED'] || false}\"\n\tundefined_name \"#{ENV['CDM_UNDEFINED_NAME'] || 'undefined'}\"\n\trename_time \"#{ENV['CDM_RENAME_TIME'] || true}\"\n\trename_time_if_missing \"#{ENV['CDM_RENAME_TIME_IF_MISSING'] || false}\"\n\tsrc_time_name \"#{ENV['CDM_SRC_TIME_NAME'] || 'time'}\"\n\tdest_time_name \"#{ENV['CDM_DEST_TIME_NAME'] || '@timestamp'}\"\n\tpipeline_type \"#{ENV['PIPELINE_TYPE'] || 'collector'}\"\n\tundefined_to_string \"#{ENV['CDM_UNDEFINED_TO_STRING'] || 'false'}\"\n\tundefined_dot_replace_char \"#{ENV['CDM_UNDEFINED_DOT_REPLACE_CHAR'] || 'UNUSED'}\"\n\tundefined_max_num_fields \"#{ENV['CDM_UNDEFINED_MAX_NUM_FIELDS'] || '-1'}\"\n\tprocess_kubernetes_events \"#{ENV['TRANSFORM_EVENTS'] || 'false'}\"\n\t\t<formatter>\n\t\ttag \"system.var.log**\"\n\t\ttype sys_var_log\n\t\tremove_keys host,pid,ident\n\t</formatter>\n\t\t<formatter>\n\t\ttag \"journal.system**\"\n\t\ttype sys_journal\n\t\tremove_keys log,stream,MESSAGE,_SOURCE_REALTIME_TIMESTAMP,__REALTIME_TIMESTAMP,CONTAINER_ID,CONTAINER_ID_FULL,CONTAINER_NAME,PRIORITY,_BOOT_ID,_CAP_EFFECTIVE,_CMDLINE,_COMM,_EXE,_GID,_HOSTNAME,_MACHINE_ID,_PID,_SELINUX_CONTEXT,_SYSTEMD_CGROUP,_SYSTEMD_SLICE,_SYSTEMD_UNIT,_TRANSPORT,_UID,_AUDIT_LOGINUID,_AUDIT_SESSION,_SYSTEMD_OWNER_UID,_SYSTEMD_SESSION,_SYSTEMD_USER_UNIT,CODE_FILE,CODE_FUNCTION,CODE_LINE,ERRNO,MESSAGE_ID,RESULT,UNIT,_KERNEL_DEVICE,_KERNEL_SUBSYSTEM,_UDEV_SYSNAME,_UDEV_DEVNODE,_UDEV_DEVLINK,SYSLOG_FACILITY,SYSLOG_IDENTIFIER,SYSLOG_PID\n\t</formatter>\n\t\t<formatter>\n\t\ttag \"kubernetes.journal.container**\"\n\t\ttype k8s_journal\n\t\tremove_keys \"#{ENV['K8S_FILTER_REMOVE_KEYS'] || 'log,stream,MESSAGE,_SOURCE_REALTIME_TIMESTAMP,__REALTIME_TIMESTAMP,CONTAINER_ID,CONTAINER_ID_FULL,CONTAINER_NAME,PRIORITY,_BOOT_ID,_CAP_EFFECTIVE,_CMDLINE,_COMM,_EXE,_GID,_HOSTNAME,_MACHINE_ID,_PID,_SELINUX_CONTEXT,_SYSTEMD_CGROUP,_SYSTEMD_SLICE,_SYSTEMD_UNIT,_TRANSPORT,_UID,_AUDIT_LOGINUID,_AUDIT_SESSION,_SYSTEMD_OWNER_UID,_SYSTEMD_SESSION,_SYSTEMD_USER_UNIT,CODE_FILE,CODE_FUNCTION,CODE_LINE,ERRNO,MESSAGE_ID,RESULT,UNIT,_KERNEL_DEVICE,_KERNEL_SUBSYSTEM,_UDEV_SYSNAME,_UDEV_DEVNODE,_UDEV_DEVLINK,SYSLOG_FACILITY,SYSLOG_IDENTIFIER,SYSLOG_PID'}\"\n\t</formatter>\n\t\t<formatter>\n\t\ttag \"kubernetes.var.log.containers.eventrouter-** kubernetes.var.log.containers.cluster-logging-eventrouter-** k8s-audit.log** openshift-audit.log**\"\n\t\ttype k8s_json_file\n\t\tremove_keys log,stream,CONTAINER_ID_FULL,CONTAINER_NAME\n\t\tprocess_kubernetes_events \"#{ENV['TRANSFORM_EVENTS'] || 'true'}\"\n\t</formatter>\n\t\t<formatter>\n\t\ttag \"kubernetes.var.log.containers**\"\n\t\ttype k8s_json_file\n\t\tremove_keys log,stream,CONTAINER_ID_FULL,CONTAINER_NAME\n\t</formatter>\n\t\t<elasticsearch_index_name>\n\t\tenabled \"#{ENV['ENABLE_ES_INDEX_NAME'] || 'true'}\"\n\t\ttag \"journal.system** system.var.log** **_default_** **_kube-*_** **_openshift-*_** **_openshift_**\"\n\t\tname_type operations_full\n\t</elasticsearch_index_name>\n\t\t<elasticsearch_index_name>\n\t\tenabled \"#{ENV['ENABLE_ES_INDEX_NAME'] || 'true'}\"\n\t\ttag \"linux-audit.log** k8s-audit.log** openshift-audit.log**\"\n\t\tname_type audit_full\n\t</elasticsearch_index_name>\n\t\t<elasticsearch_index_name>\n\t\tenabled \"#{ENV['ENABLE_ES_INDEX_NAME'] || 'true'}\"\n\t\ttag \"**\"\n\t\tname_type project_full\n\t</elasticsearch_index_name>\n</filter>\n\t<filter **>\n\t@type elasticsearch_genid_ext\n\thash_id_key viaq_msg_id\n\talt_key kubernetes.event.metadata.uid\n\talt_tags \"#{ENV['GENID_ALT_TAG'] || 'kubernetes.var.log.containers.logging-eventrouter-*.** kubernetes.var.log.containers.eventrouter-*.** kubernetes.var.log.containers.cluster-logging-eventrouter-*.** kubernetes.journal.container._default_.kubernetes-event'}\"\n</filter>\n\n# Relabel specific source tags to specific intermediary labels for copy processing\n\n\t<match **_default_** **_kube-*_** **_openshift-*_** **_openshift_** journal.** system.var.log**>\n\t@type relabel\n\t@label @_LOGS_INFRA\n</match>\n\n\t<match kubernetes.**>\n\t@type relabel\n\t@label @_LOGS_APP\n</match>\n\n\t<match **>\n\t@type stdout\n</match>\n\n</label>\n\n# Relabel specific sources (e.g. logs.apps) to multiple pipelines\n\n<label @_LOGS_APP>\n\t<match **>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type relabel\n\t\t@label @CLO_DEFAULT_APP_PIPELINE\n\t</store>\n\t\n\t\n</match>\n</label>\n\n<label @_LOGS_INFRA>\n\t<match **>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type relabel\n\t\t@label @CLO_DEFAULT_INFRA_PIPELINE\n\t</store>\n\t\n\t\n</match>\n</label>\n\n# Relabel specific pipelines to multiple, outputs (e.g. ES, kafka stores)\n\n<label @CLO_DEFAULT_APP_PIPELINE>\n\t<match **>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type relabel\n\t\t@label @CLO_DEFAULT_OUTPUT_ES\n\t</store>\n</match>\n</label>\n\n<label @CLO_DEFAULT_INFRA_PIPELINE>\n\t<match **>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type relabel\n\t\t@label @CLO_DEFAULT_OUTPUT_ES\n\t</store>\n</match>\n</label>\n\n# Ship logs to specific outputs\n\n<label @CLO_DEFAULT_OUTPUT_ES>\n\t<match retry_clo_default_output_es>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type elasticsearch\n\t\t@id retry_clo_default_output_es\n\t\thost elasticsearch.openshift-logging.svc.cluster.local\n\t\tport 9200\n\t\tscheme https\n\t\tssl_version TLSv1_2\n\t\ttarget_index_key viaq_index_name\n\t\tid_key viaq_msg_id\n\t\tremove_keys viaq_index_name\n\t\tuser fluentd\n\t\tpassword changeme\n\t\t\n\t\tclient_key '/var/run/ocp-collector/secrets/fluentd/tls.key'\n\t\tclient_cert '/var/run/ocp-collector/secrets/fluentd/tls.crt'\n\t\tca_file '/var/run/ocp-collector/secrets/fluentd/ca-bundle.crt'\n\t\ttype_name com.redhat.viaq.common\n\t\twrite_operation create\n\t\treload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'true'}\"\n\t\t# https://github.com/uken/fluent-plugin-elasticsearch#reload-after\n\t\treload_after \"#{ENV['ES_RELOAD_AFTER'] || '200'}\"\n\t\t# https://github.com/uken/fluent-plugin-elasticsearch#sniffer-class-name\n\t\tsniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::Plugin::ElasticsearchSimpleSniffer'}\"\n\t\treload_on_failure false\n\t\t# 2 ^ 31\n\t\trequest_timeout 2147483648\n\t\t\t<buffer>\n\t\t\t@type file\n\t\t\tpath '/var/lib/fluentd/retry_clo_default_output_es'\n\t\t\tflush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '1s'}\"\n\t\t\tflush_thread_count \"#{ENV['ES_FLUSH_THREAD_COUNT'] || 2}\"\n\t\t\tflush_at_shutdown \"#{ENV['FLUSH_AT_SHUTDOWN'] || 'false'}\"\n\t\t\tretry_max_interval \"#{ENV['ES_RETRY_WAIT'] || '300'}\"\n\t\t\tretry_forever true\n\t\t\tqueue_limit_length \"#{ENV['BUFFER_QUEUE_LIMIT'] || '32' }\"\n\t\t\tchunk_limit_size \"#{ENV['BUFFER_SIZE_LIMIT'] || '8m' }\"\n\t\t\toverflow_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'block'}\"\n\t\t</buffer>\n\t</store>\n</match>\n\t<match **>\n\t@type copy\n\t\n\t\t<store>\n\t\t@type elasticsearch\n\t\t@id clo_default_output_es\n\t\thost elasticsearch.openshift-logging.svc.cluster.local\n\t\tport 9200\n\t\tscheme https\n\t\tssl_version TLSv1_2\n\t\ttarget_index_key viaq_index_name\n\t\tid_key viaq_msg_id\n\t\tremove_keys viaq_index_name\n\t\tuser fluentd\n\t\tpassword changeme\n\t\t\n\t\tclient_key '/var/run/ocp-collector/secrets/fluentd/tls.key'\n\t\tclient_cert '/var/run/ocp-collector/secrets/fluentd/tls.crt'\n\t\tca_file '/var/run/ocp-collector/secrets/fluentd/ca-bundle.crt'\n\t\ttype_name com.redhat.viaq.common\n\t\tretry_tag retry_clo_default_output_es\n\t\twrite_operation create\n\t\treload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'true'}\"\n\t\t# https://github.com/uken/fluent-plugin-elasticsearch#reload-after\n\t\treload_after \"#{ENV['ES_RELOAD_AFTER'] || '200'}\"\n\t\t# https://github.com/uken/fluent-plugin-elasticsearch#sniffer-class-name\n\t\tsniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::Plugin::ElasticsearchSimpleSniffer'}\"\n\t\treload_on_failure false\n\t\t# 2 ^ 31\n\t\trequest_timeout 2147483648\n\t\t\t<buffer>\n\t\t\t@type file\n\t\t\tpath '/var/lib/fluentd/clo_default_output_es'\n\t\t\tflush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '1s'}\"\n\t\t\tflush_thread_count \"#{ENV['ES_FLUSH_THREAD_COUNT'] || 2}\"\n\t\t\tflush_at_shutdown \"#{ENV['FLUSH_AT_SHUTDOWN'] || 'false'}\"\n\t\t\tretry_max_interval \"#{ENV['ES_RETRY_WAIT'] || '300'}\"\n\t\t\tretry_forever true\n\t\t\tqueue_limit_length \"#{ENV['BUFFER_QUEUE_LIMIT'] || '32' }\"\n\t\t\tchunk_limit_size \"#{ENV['BUFFER_SIZE_LIMIT'] || '8m' }\"\n\t\t\toverflow_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'block'}\"\n\t\t</buffer>\n\t</store>\n</match>\n</label>\n\n\n"
  run.sh: >
    #!/bin/bash


    export MERGE_JSON_LOG=${MERGE_JSON_LOG:-false}

    CFG_DIR=/etc/fluent/configs.d

    ENABLE_PROMETHEUS_ENDPOINT=${ENABLE_PROMETHEUS_ENDPOINT:-"true"}

    OCP_OPERATIONS_PROJECTS=${OCP_OPERATIONS_PROJECTS:-"default openshift
    openshift- kube-"}


    fluentdargs="--no-supervisor"

    # find the sniffer class file

    sniffer=$( gem contents fluent-plugin-elasticsearch|grep
    elasticsearch_simple_sniffer.rb )

    if [ -z "$sniffer" ] ; then
        sniffer=$( rpm -ql rubygem-fluent-plugin-elasticsearch|grep elasticsearch_simple_sniffer.rb )
    fi

    if [ -n "$sniffer" -a -f "$sniffer" ] ; then
        fluentdargs="$fluentdargs -r $sniffer"
    fi


    if [[ $VERBOSE ]]; then
      set -ex
      fluentdargs="$fluentdargs -vv --log-event-verbose"
      echo ">>>>>> ENVIRONMENT VARS <<<<<"
      env | sort
      echo ">>>>>>>>>>>>><<<<<<<<<<<<<<<<"
    else
      set -e
      fluentdargs="-q --suppress-config-dump $fluentdargs"
    fi



    issue_deprecation_warnings() {
        : # none at the moment
    }


    if [ -z "${JOURNAL_SOURCE:-}" ] ; then
        if [ -d /var/log/journal ] ; then
            export JOURNAL_SOURCE=/var/log/journal
        else
            export JOURNAL_SOURCE=/run/log/journal
        fi
    fi


    IPADDR4=${NODE_IPV4:-$( /usr/sbin/ip -4 addr show dev eth0 | grep inet | sed
    -e "s/[ \t]*inet \([0-9.]*\).*/\1/" )}

    IPADDR6=${NODE_IPV6:-$(/usr/sbin/ip -6 addr show dev eth0 | grep inet | sed
    -e "s/[ \t]*inet6 \([a-z0-9::]*\).*/\1/" )}


    export IPADDR4 IPADDR6


    BUFFER_SIZE_LIMIT=${BUFFER_SIZE_LIMIT:-16777216}


    # Generate throttle configs and outputs

    ruby generate_throttle_configs.rb

    # have output plugins handle back pressure

    # if you want the old behavior to be forced anyway, set env

    # BUFFER_QUEUE_FULL_ACTION=exception

    export BUFFER_QUEUE_FULL_ACTION=${BUFFER_QUEUE_FULL_ACTION:-block}


    # this is the list of keys to remove when the record is transformed from the
    raw systemd journald

    # output to the viaq data model format

    K8S_FILTER_REMOVE_KEYS="log,stream,MESSAGE,_SOURCE_REALTIME_TIMESTAMP,__REALTIME_TIMESTAMP,CONTAINER_ID,CONTAINER_ID_FULL,CONTAINER_NAME,PRIORITY,_BOOT_ID,_CAP_EFFECTIVE,_CMDLINE,_COMM,_EXE,_GID,_HOSTNAME,_MACHINE_ID,_PID,_SELINUX_CONTEXT,_SYSTEMD_CGROUP,_SYSTEMD_SLICE,_SYSTEMD_UNIT,_TRANSPORT,_UID,_AUDIT_LOGINUID,_AUDIT_SESSION,_SYSTEMD_OWNER_UID,_SYSTEMD_SESSION,_SYSTEMD_USER_UNIT,CODE_FILE,CODE_FUNCTION,CODE_LINE,ERRNO,MESSAGE_ID,RESULT,UNIT,_KERNEL_DEVICE,_KERNEL_SUBSYSTEM,_UDEV_SYSNAME,_UDEV_DEVNODE,_UDEV_DEVLINK,SYSLOG_FACILITY,SYSLOG_IDENTIFIER,SYSLOG_PID"

    export K8S_FILTER_REMOVE_KEYS ENABLE_ES_INDEX_NAME


    # Check bearer_token_file for fluent-plugin-kubernetes_metadata_filter.

    if [ ! -s /var/run/secrets/kubernetes.io/serviceaccount/token ] ; then
        echo "ERROR: Bearer_token_file (/var/run/secrets/kubernetes.io/serviceaccount/token) to access the Kubernetes API server is missing or empty."
        exit 1
    fi


    # If FILE_BUFFER_PATH exists and it is not a directory, mkdir fails with the
    error.

    FILE_BUFFER_PATH=/var/lib/fluentd

    mkdir -p $FILE_BUFFER_PATH


    FLUENT_CONF=$CFG_DIR/user/fluent.conf

    if [ ! -f "$FLUENT_CONF" ] ; then
        echo "ERROR: The configuration $FLUENT_CONF does not exist"
        exit 1
    fi

    NUM_OUTPUTS=$(grep "path.*'$FILE_BUFFER_PATH" $FLUENT_CONF | wc -l)


    # Get the available disk size.

    DF_LIMIT=$(df -B1 $FILE_BUFFER_PATH | grep -v Filesystem | awk '{print $2}')

    DF_LIMIT=${DF_LIMIT:-0}

    if [ $DF_LIMIT -eq 0 ]; then
        echo "ERROR: No disk space is available for file buffer in $FILE_BUFFER_PATH."
        exit 1
    fi

    # Determine final total given the number of outputs we have.

    TOTAL_LIMIT=$(echo ${FILE_BUFFER_LIMIT:-2Gi} | sed -e
    "s/[Kk]/*1024/g;s/[Mm]/*1024*1024/g;s/[Gg]/*1024*1024*1024/g;s/i//g" | bc)
    || :

    if [ $TOTAL_LIMIT -le 0 ]; then
        echo "ERROR: Invalid file buffer limit ($FILE_BUFFER_LIMIT) is given.  Failed to convert to bytes."
        exit 1
    fi


    TOTAL_LIMIT=$(expr $TOTAL_LIMIT \* $NUM_OUTPUTS) || :

    if [ $DF_LIMIT -lt $TOTAL_LIMIT ]; then
        echo "WARNING: Available disk space ($DF_LIMIT bytes) is less than the user specified file buffer limit ($FILE_BUFFER_LIMIT times $NUM_OUTPUTS)."
        TOTAL_LIMIT=$DF_LIMIT
    fi


    BUFFER_SIZE_LIMIT=$(echo $BUFFER_SIZE_LIMIT |  sed -e
    "s/[Kk]/*1024/g;s/[Mm]/*1024*1024/g;s/[Gg]/*1024*1024*1024/g;s/i//g" | bc)

    BUFFER_SIZE_LIMIT=${BUFFER_SIZE_LIMIT:-16777216}


    # TOTAL_BUFFER_SIZE_LIMIT per buffer

    TOTAL_BUFFER_SIZE_LIMIT=$(expr $TOTAL_LIMIT / $NUM_OUTPUTS) || :

    if [ -z $TOTAL_BUFFER_SIZE_LIMIT -o $TOTAL_BUFFER_SIZE_LIMIT -eq 0 ]; then
        echo "ERROR: Calculated TOTAL_BUFFER_SIZE_LIMIT is 0. TOTAL_LIMIT $TOTAL_LIMIT is too small compared to NUM_OUTPUTS $NUM_OUTPUTS. Please increase FILE_BUFFER_LIMIT $FILE_BUFFER_LIMIT and/or the volume size of $FILE_BUFFER_PATH."
        exit 1
    fi

    BUFFER_QUEUE_LIMIT=$(expr $TOTAL_BUFFER_SIZE_LIMIT / $BUFFER_SIZE_LIMIT) ||
    :

    if [ -z $BUFFER_QUEUE_LIMIT -o $BUFFER_QUEUE_LIMIT -eq 0 ]; then
        echo "ERROR: Calculated BUFFER_QUEUE_LIMIT is 0. TOTAL_BUFFER_SIZE_LIMIT $TOTAL_BUFFER_SIZE_LIMIT is too small compared to BUFFER_SIZE_LIMIT $BUFFER_SIZE_LIMIT. Please increase FILE_BUFFER_LIMIT $FILE_BUFFER_LIMIT and/or the volume size of $FILE_BUFFER_PATH."
        exit 1
    fi

    export BUFFER_QUEUE_LIMIT BUFFER_SIZE_LIMIT


    # http://docs.fluentd.org/v0.12/articles/monitoring

    if [ "${ENABLE_MONITOR_AGENT:-}" = true ] ; then
        cp $CFG_DIR/input-pre-monitor.conf $CFG_DIR/openshift
        # copy any user defined files, possibly overwriting the standard ones
        if [ -f $CFG_DIR/user/input-pre-monitor.conf ] ; then
            cp -f $CFG_DIR/user/input-pre-monitor.conf $CFG_DIR/openshift
        fi
    else
        rm -f $CFG_DIR/openshift/input-pre-monitor.conf
    fi


    # http://docs.fluentd.org/v0.12/articles/monitoring#debug-port

    if [ "${ENABLE_DEBUG_AGENT:-}" = true ] ; then
        cp $CFG_DIR/input-pre-debug.conf $CFG_DIR/openshift
        # copy any user defined files, possibly overwriting the standard ones
        if [ -f $CFG_DIR/user/input-pre-debug.conf ] ; then
            cp -f $CFG_DIR/user/input-pre-debug.conf $CFG_DIR/openshift
        fi
    else
        rm -f $CFG_DIR/openshift/input-pre-debug.conf
    fi


    # bug https://bugzilla.redhat.com/show_bug.cgi?id=1437952

    # pods unable to be terminated because fluentd has them busy

    if [ -d /var/lib/docker/containers ] ; then
        # If oci-umount is fixed, we can remove this.
        if [ -n "${VERBOSE:-}" ] ; then
            echo "umounts of dead containers will fail. Ignoring..."
            umount /var/lib/docker/containers/*/shm || :
        else
            umount /var/lib/docker/containers/*/shm > /dev/null 2>&1 || :
        fi
    fi


    if [ "${AUDIT_CONTAINER_ENGINE:-}" = "true" ] ; then
        cp -f $CFG_DIR/input-pre-audit-log.conf $CFG_DIR/openshift
        cp -f $CFG_DIR/filter-pre-a-audit-exclude.conf $CFG_DIR/openshift
    else
        touch $CFG_DIR/openshift/input-pre-audit-log.conf
        touch $CFG_DIR/openshift/filter-pre-a-audit-exclude.conf
    fi


    if [ "${ENABLE_UTF8_FILTER:-}" != true ] ; then
        rm -f $CFG_DIR/openshift/filter-pre-force-utf8.conf
        touch $CFG_DIR/openshift/filter-pre-force-utf8.conf
    fi


    # Include DEBUG log level messages when collecting from journald

    # https://bugzilla.redhat.com/show_bug.cgi?id=1505602

    if [ "${COLLECT_JOURNAL_DEBUG_LOGS:-true}" = true ] ; then
      rm -f $CFG_DIR/openshift/filter-exclude-journal-debug.conf
      touch $CFG_DIR/openshift/filter-exclude-journal-debug.conf
    fi


    if [ "${ENABLE_PROMETHEUS_ENDPOINT}" != "true" ] ; then
      echo "INFO: Disabling Prometheus endpoint"
      rm -f ${CFG_DIR}/openshift/input-pre-prometheus-metrics.conf
    fi


    # convert journal.pos file to new format

    if [ -f /var/log/journal.pos -a ! -f /var/log/journal_pos.json ] ; then
        echo Converting old fluent-plugin-systemd pos file format to new format
        cursor=$( cat /var/log/journal.pos )
        echo '{"journal":"'"$cursor"'"}' > /var/log/journal_pos.json
        rm /var/log/journal.pos
    fi


    issue_deprecation_warnings


    # this should be the last thing before launching fluentd so as not to use

    # jemalloc with any other processes

    if type -p jemalloc-config > /dev/null 2>&1 && [ "${USE_JEMALLOC:-true}" =
    true ] ; then
        export LD_PRELOAD=$( jemalloc-config --libdir )/libjemalloc.so.$( jemalloc-config --revision )
        export LD_BIND_NOW=1 # workaround for https://bugzilla.redhat.com/show_bug.cgi?id=1544815
    fi

    exec fluentd $fluentdargs
  throttle-config.yaml: |
    # Logging example fluentd throttling config file

    #example-project:
    #  read_lines_limit: 10
    #
    #.operations:
    #  read_lines_limit: 100
